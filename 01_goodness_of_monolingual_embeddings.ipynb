{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you evaluate monolingual embeddings?\n",
    "\n",
    "What constitutes a good embedding? It would be easy to provide a cursory answer - a good embedding is one that lends itself well to a downstream task. While 100% true and accurate, this answer does not allow us to speak to the goodness of embeddings directly, without having to train an additional model.\n",
    "\n",
    "Another way to look at this would be to define that embeddings are valuable and useful to the extent that they encode information about the language and the world around us. This is in line with the reasoning behind why embeddings were created in the first place - we want to train our embeddings, or our language model, on vast amount of unlabeled text, in a way that encodes syntactic and semantic information that can give us a boost on a downstream task where we have labels but the dataset might be of limited size.\n",
    "\n",
    "Taking the second definition, we can attempt to query our embeddings on textual examples and evaluate the accuracy of the anwsers. In its simplest form, we can perform algebraic operations in the embedding space (\"king\" - \"man\" + \"woman\" = ?) and use this as a mechanism for evaluation. While not without [issues](https://www.aclweb.org/anthology/W16-2503.pdf), this approach does allow us to say something about the structure of the trained embedding space.\n",
    "\n",
    "To demonstrate this approach, let's use the embeddings from a classic, seminal paper [Efficient Estimation of Word Representations in Vector Space paper](https://arxiv.org/abs/1301.3781) by Tomas Mikolov et al.\n",
    "\n",
    "We can download the embeddings using fastai (they were originally shared by the authors [here](https://code.google.com/archive/p/word2vec/)).\n",
    "\n",
    "Please note - the file is 1.7 GB compressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import untar_data\n",
    "embedding_path = untar_data('https://storage.googleapis.com/text-embeddings/GoogleNews-vectors-negative300.bin.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the embeddings using `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "gensim_embeddings = KeyedVectors.load_word2vec_format(embedding_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, (300,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_embeddings.index2entity), gensim_embeddings['cat'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 million distinct embeddings, each of dimensionality 300!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the evaluation using the original list of `question-words.txt` as used in the paper (and that was shared by the authors on github [here](https://github.com/tmikolov/word2vec/blob/master/questions-words.txt)).\n",
    "\n",
    "We could use the functionality built into `gensim` to run the evaluation, but this might make it tricky to evaluate embeddings that we train ourselves, or should we want to modify the list of queries.\n",
    "\n",
    "Instead, let's perform the evaluation using code that we develop in this repository. As a starting point, all we need is an array of embeddings and a list with words corresponding to each vector!\n",
    "\n",
    "<!-- We will use [annoy](https://github.com/spotify/annoy) for approximate nearest neighbor lookup. Upon the first run, the embeddings will be added to an index and multiple trees enabling the search will be constructed. Given the size of these embeddings, this took around 5 minutes for me.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import numpy as np\n",
    "\n",
    "class Embeddings():\n",
    "    def __init__(self, embeddings, index2word):\n",
    "        '''embeddings - numpy array of embeddings, index2word - list of words corresponding to embeddings'''\n",
    "        assert len(embeddings) == len(index2word)\n",
    "        self.vectors = embeddings\n",
    "        self.i2w = index2word\n",
    "        self.w2i = {w:i for i, w in enumerate(index2word)}\n",
    "            \n",
    "    def analogy(self, a, b, c, n=5, discard_question_words=True):\n",
    "        '''\n",
    "        a is to b as c is to ?\n",
    "        \n",
    "        Performs the following algebraic calculation: result = emb_a - emb_b + emb_c\n",
    "        Looks up n closest words to result.\n",
    "        \n",
    "        Implements the embedding space math behind the famous word2vec example:\n",
    "        king - man + woman = queen\n",
    "        '''\n",
    "        question_word_indices = [self.w2i[word] for word in [a, b, c]]\n",
    "        a, b, c = [self.vectors[idx] for idx in question_word_indices] \n",
    "        result = a - b + c\n",
    "        \n",
    "        if discard_question_words: return self.nn_words_to(result, question_word_indices, n)\n",
    "        else:                      return self.nn_words_to(result, n=n)\n",
    "        \n",
    "    def nn_words_to(self, vector, skip_indices=[], n=5):\n",
    "        nn_indices = self.word_idxs_ranked_by_cosine_similarity_to(vector)\n",
    "        nn_words = []\n",
    "        for idx in nn_indices:\n",
    "            if idx in skip_indices: continue\n",
    "            nn_words.append(self.i2w[idx])\n",
    "            if len(nn_words) == n: break\n",
    "        \n",
    "        return nn_words\n",
    "    \n",
    "    def word_idxs_ranked_by_cosine_similarity_to(self, vector):\n",
    "        return np.flip(\n",
    "            np.argsort(self.vectors @ vector / (self.vectors_lengths() * np.linalg.norm(vector, axis=-1)))\n",
    "        )\n",
    "    \n",
    "    def vectors_lengths(self):\n",
    "        if not hasattr(self, 'vectors_length_cache'):\n",
    "            self.vectors_length_cache = np.linalg.norm(self.vectors, axis=-1)\n",
    "        return self.vectors_length_cache\n",
    "    \n",
    "    def __getitem__(self, word):\n",
    "        return self.vectors[self.w2i[word]]\n",
    "    \n",
    "    @classmethod\n",
    "    def from_txt_file(cls, path_to_txt_file, limit=None):\n",
    "        '''create embeddings from word2vec embeddings text file'''\n",
    "        index, vectors = [], []\n",
    "        with open(path_to_txt_file) as f:\n",
    "            f.readline() # discarding the header line\n",
    "            for line in f:\n",
    "                try:\n",
    "                    embedding = np.array([float(s) for s in line.split()[1:]])\n",
    "                    if embedding.shape[0] != 300: continue\n",
    "                    vectors.append(embedding)\n",
    "                    index.append(line.split()[0])\n",
    "                except ValueError: pass # we may have encountered a 2 word embedding, for instance 'New York' or 'w dolinie'\n",
    "                if limit is not None and len(vectors) == limit: break\n",
    "        return cls(np.stack(vectors), index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_embeddings.vectors[:30000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing just the vectors and mapping of vectors to words from gensim embeddings and instatiating our own embedding object\n",
    "# let's stick to just 50_000 of the most popular words so that the computation will run faster\n",
    "\n",
    "embeddings = Embeddings(gensim_embeddings.vectors[:50_000], gensim_embeddings.index2word[:50_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Embeddings in place, we can run some examples. France is to Paris as ? is to Warsaw..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 24 ms, total: 132 ms\n",
      "Wall time: 33.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Poland', 'Polish', 'Romania', 'Lithuania', 'Poles']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embeddings.analogy('France', 'Paris', 'Warsaw', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got that one right! Now let's try the classic example of king - man + women = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32 ms, sys: 0 ns, total: 32 ms\n",
      "Wall time: 7.17 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['queen', 'monarch', 'princess', 'prince', 'kings']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embeddings.analogy('king', 'man', 'woman', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get it right as well!\n",
    "\n",
    "Despite kings and queens not being discussed that often in the news today, this is still a great and slightly unexpected performance. Why should such an algebraic structure emerge when trained on a lot of text data in the first place? But yet it does!\n",
    "\n",
    "Let's explore the performance further, by running through the list of question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_monolingual_embeddings(embeddings, lower=False):\n",
    "    with open('data/questions-words.txt') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    total_seen = defaultdict(lambda: 0)\n",
    "    correct = defaultdict(lambda: 0)\n",
    "    question_types = []\n",
    "    not_found = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if line[0] == ':':\n",
    "            question_types.append(line[1:].strip())\n",
    "            current_type = question_types[-1]\n",
    "        else:\n",
    "            total_seen[current_type] += 1\n",
    "            example = line.strip().split(' ')\n",
    "            if lower: example = [word.lower() for word in example]\n",
    "            try:\n",
    "                result = embeddings.analogy(*example[:2], example[3], 1)\n",
    "                if example[2] == result[0]: correct[current_type] += 1\n",
    "            except KeyError:\n",
    "                not_found += 1\n",
    "            \n",
    "    types = []\n",
    "    results = []\n",
    "    for key in total_seen.keys():\n",
    "        types.append(key)\n",
    "        results.append(f'{correct[key]} / {total_seen[key]}')\n",
    "        \n",
    "    df = pd.DataFrame(data={'question type': types, 'result': results})\n",
    "    display(df)\n",
    "    print('Accuracy:', sum(correct.values()) / sum(total_seen.values()))\n",
    "    print('Examples with missing words in the dictionary:', not_found)\n",
    "    print('Total examples:', sum(total_seen.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question type</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capital-common-countries</td>\n",
       "      <td>397 / 506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capital-world</td>\n",
       "      <td>1512 / 4524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>currency</td>\n",
       "      <td>64 / 866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city-in-state</td>\n",
       "      <td>463 / 2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>family</td>\n",
       "      <td>377 / 506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gram1-adjective-to-adverb</td>\n",
       "      <td>257 / 992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gram2-opposite</td>\n",
       "      <td>257 / 812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>1044 / 1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>644 / 1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gram5-present-participle</td>\n",
       "      <td>745 / 1056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>1401 / 1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gram7-past-tense</td>\n",
       "      <td>1139 / 1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gram8-plural</td>\n",
       "      <td>876 / 1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gram9-plural-verbs</td>\n",
       "      <td>570 / 870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question type       result\n",
       "0      capital-common-countries    397 / 506\n",
       "1                 capital-world  1512 / 4524\n",
       "2                      currency     64 / 866\n",
       "3                 city-in-state   463 / 2467\n",
       "4                        family    377 / 506\n",
       "5     gram1-adjective-to-adverb    257 / 992\n",
       "6                gram2-opposite    257 / 812\n",
       "7             gram3-comparative  1044 / 1332\n",
       "8             gram4-superlative   644 / 1122\n",
       "9      gram5-present-participle   745 / 1056\n",
       "10  gram6-nationality-adjective  1401 / 1599\n",
       "11             gram7-past-tense  1139 / 1560\n",
       "12                 gram8-plural   876 / 1332\n",
       "13           gram9-plural-verbs    570 / 870"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4986696684404421\n",
      "Examples with missing words in the dictionary: 3175\n",
      "Total examples: 19544\n",
      "CPU times: user 5min 54s, sys: 3.6 s, total: 5min 57s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "evaluate_monolingual_embeddings(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very good result - bear in mind that we are limiting ourselves to top@1 accuracy and that we are counting synonyms as failure!\n",
    "\n",
    "Another consideration is that while word2vec embeddings are ingenious in how efficient they are to train, they are a relatively simple way of encoding information about a language. \n",
    "\n",
    "Still, it is remarkable that embedding spaces posses the quality that allows us to perform operations such as the above!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
