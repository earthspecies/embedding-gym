{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you evaluate goodness of embeddings across languages?\n",
    "\n",
    "If you have embeddings for language A and another set of embeddings for language B, how can you tell how good they are? \n",
    "\n",
    "We might want to evaluate each set on a task specific to that language. But a much more interesting proposition is aligning the embeddings in an unsupervised way and evaluating the performance on translation!\n",
    "\n",
    "For this, we need 3 components. [The embeddings](https://fasttext.cc/docs/en/pretrained-vectors.html), [a way to align embeddings](https://github.com/artetxem/vecmap) and [dictionaries](https://github.com/facebookresearch/MUSE) we can use to evaluate the results.\n",
    "\n",
    "Let's begin by grabbing embeddings for English and Polish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-14 13:54:07--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6597238061 (6.1G) [binary/octet-stream]\n",
      "Saving to: ‘data/wiki.en.vec’\n",
      "\n",
      "wiki.en.vec         100%[===================>]   6.14G  44.6MB/s    in 3m 2s   \n",
      "\n",
      "2020-09-14 13:57:09 (34.6 MB/s) - ‘data/wiki.en.vec’ saved [6597238061/6597238061]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P data https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-14 13:57:09--  https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pl.vec\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 172.67.9.4, 104.22.74.142, 104.22.75.142, ...\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|172.67.9.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2707436342 (2.5G) [binary/octet-stream]\n",
      "Saving to: ‘data/wiki.pl.vec’\n",
      "\n",
      "wiki.pl.vec         100%[===================>]   2.52G  43.3MB/s    in 74s     \n",
      "\n",
      "2020-09-14 13:58:24 (34.9 MB/s) - ‘data/wiki.pl.vec’ saved [2707436342/2707436342]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -P data https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pl.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to align the embeddings. Let's translate from Polish to English. In order to do that, let's align our Polish embeddings with our English embeddings.\n",
    "\n",
    "Bear in mind that this is a challenging task - Polish and English come from distinct and very different language families (West Slavic languages of the Lechitic group and West Germanic language of the Indo-European language family respectively).\n",
    "\n",
    "Let us clone [vecmap](https://github.com/artetxem/vecmap) to a directory adjacent to this one and perform the alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/wiki.pl.vec') as f:\n",
    "    en_src = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032578"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.data.all import untar_data\n",
    "embedding_path = untar_data('https://storage.googleapis.com/text-embeddings/GoogleNews-vectors-negative300.bin.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the embeddings using `gensim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "gensim_embeddings = KeyedVectors.load_word2vec_format(embedding_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000000, (300,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_embeddings.index2entity), gensim_embeddings['cat'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 million distinct embeddings, each of dimensionality 300!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform the evaluation using the original list of `question-words.txt` as used in the paper (and that was shared by the authors on github [here](https://github.com/tmikolov/word2vec/blob/master/questions-words.txt)).\n",
    "\n",
    "We could use the functionality built into `gensim` to run the evaluation, but this might make it tricky to evaluate embeddings that we train ourselves, or should we want to modify the list of queries.\n",
    "\n",
    "Instead, let's perform the evaluation using code that we develop in this repository. As a starting point, all we need is an array of embeddings and a list with words corresponding to each vector!\n",
    "\n",
    "<!-- We will use [annoy](https://github.com/spotify/annoy) for approximate nearest neighbor lookup. Upon the first run, the embeddings will be added to an index and multiple trees enabling the search will be constructed. Given the size of these embeddings, this took around 5 minutes for me.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Embeddings():\n",
    "    def __init__(self, embeddings, index2word):\n",
    "        '''embeddings - numpy array of embeddings, index2word - list of words corresponding to embeddings'''\n",
    "        self.vectors = embeddings\n",
    "        self.i2w = index2word\n",
    "        self.w2i = {w:i for i, w in enumerate(index2word)}\n",
    "            \n",
    "    def analogy(self, a, b, c, n=5):\n",
    "        '''\n",
    "        a is to b as c is to ?\n",
    "        \n",
    "        Performs the following algebraic calculation: result = emb_a - emb_b + emb_c\n",
    "        Looks up n closest words to result.\n",
    "        \n",
    "        Implements the embedding space math behind the famous word2vec example:\n",
    "        king - man + woman = queen\n",
    "        '''\n",
    "        question_word_indices = [self.w2i[word] for word in [a, b, c]]\n",
    "        a, b, c = [self.vectors[idx] for idx in question_word_indices] \n",
    "        result = a - b + c\n",
    "        \n",
    "        nn_indices = np.flip(\n",
    "            np.argsort(self.vectors @ result / (np.linalg.norm(self.vectors, axis=1) * np.linalg.norm(result)))\n",
    "        )\n",
    "        \n",
    "        nn_words = []\n",
    "        for idx in nn_indices:\n",
    "            if idx in question_word_indices: continue\n",
    "            nn_words.append(self.i2w[idx])\n",
    "            if len(nn_words) == n: break\n",
    "        \n",
    "        return nn_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_embeddings.vectors[:30000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing just the vectors and mapping of vectors to words from gensim embeddings and instatiating our own embedding object\n",
    "# let's stick to just 50_000 of the most popular words so that the computation will run faster\n",
    "\n",
    "embeddings = Embeddings(gensim_embeddings.vectors[:50_000], gensim_embeddings.index2word[:50_000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the Embeddings in place, we can run some examples. France is to Paris as ? is to Warsaw..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 4 ms, total: 120 ms\n",
      "Wall time: 39.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Poland', 'Polish', 'Romania', 'Lithuania', 'Poles']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embeddings.analogy('France', 'Paris', 'Warsaw', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got that one right! Now let's try the classic example of king - man + women = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 ms, sys: 12 ms, total: 108 ms\n",
      "Wall time: 36 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['queen', 'monarch', 'princess', 'prince', 'kings']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embeddings.analogy('king', 'man', 'woman', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get it right as well!\n",
    "\n",
    "Despite kings and queens not being discussed that often in the news today, this is still a great and slightly unexpected performance. Why should such an algebraic structure emerge when trained on a lot of text data in the first place? But yet it does!\n",
    "\n",
    "Let's explore the performance further, by running through the list of question-answer pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/questions-words.txt') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24min 59s, sys: 3min 26s, total: 28min 26s\n",
      "Wall time: 9min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "total_seen = defaultdict(lambda: 0)\n",
    "correct = defaultdict(lambda: 0)\n",
    "question_types = []\n",
    "\n",
    "for line in lines:\n",
    "    if line[0] == ':':\n",
    "        question_types.append(line[1:].strip())\n",
    "        current_type = question_types[-1]\n",
    "    else:\n",
    "        total_seen[current_type] += 1\n",
    "        example = line.strip().split(' ')\n",
    "        try:\n",
    "            result = embeddings.analogy(*example[:2], example[3], 1)\n",
    "            if example[2] == result[0]: correct[current_type] += 1\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "results = []\n",
    "for key in total_seen.keys():\n",
    "    types.append(key)\n",
    "    results.append(f'{correct[key]} / {total_seen[key]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question type</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capital-common-countries</td>\n",
       "      <td>397 / 506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capital-world</td>\n",
       "      <td>1512 / 4524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>currency</td>\n",
       "      <td>64 / 866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city-in-state</td>\n",
       "      <td>463 / 2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>family</td>\n",
       "      <td>377 / 506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gram1-adjective-to-adverb</td>\n",
       "      <td>257 / 992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gram2-opposite</td>\n",
       "      <td>257 / 812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>1044 / 1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>644 / 1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gram5-present-participle</td>\n",
       "      <td>745 / 1056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>1401 / 1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gram7-past-tense</td>\n",
       "      <td>1139 / 1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gram8-plural</td>\n",
       "      <td>876 / 1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gram9-plural-verbs</td>\n",
       "      <td>570 / 870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question type       result\n",
       "0      capital-common-countries    397 / 506\n",
       "1                 capital-world  1512 / 4524\n",
       "2                      currency     64 / 866\n",
       "3                 city-in-state   463 / 2467\n",
       "4                        family    377 / 506\n",
       "5     gram1-adjective-to-adverb    257 / 992\n",
       "6                gram2-opposite    257 / 812\n",
       "7             gram3-comparative  1044 / 1332\n",
       "8             gram4-superlative   644 / 1122\n",
       "9      gram5-present-participle   745 / 1056\n",
       "10  gram6-nationality-adjective  1401 / 1599\n",
       "11             gram7-past-tense  1139 / 1560\n",
       "12                 gram8-plural   876 / 1332\n",
       "13           gram9-plural-verbs    570 / 870"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4986696684404421\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data={'question type': types, 'result': results})\n",
    "display(df)\n",
    "print('Accuracy:', sum(correct.values()) / sum(total_seen.values()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
