{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our own embeddings\n",
    "\n",
    "In this notebook we will train our own text embeddings and subsequently put them through evaluation using code we wrote in the earlier notebooks.\n",
    "\n",
    "To train our embeddings let us use [fastai](https://github.com/fastai/fastai).\n",
    "\n",
    "I looked for the Google News corpus dataset, the one that word2vec embeddings were trained on, but I can't find it! There is some chance it was not shared by Google or that for one reason or another it was taken down.\n",
    "\n",
    "Let's use the next best alternative for our data, that is wikipedia dumps.\n",
    "\n",
    "We will use data generously shared by authors of [MultiFiT: Efficient Multi-lingual Language Model Fine-tuning](https://arxiv.org/abs/1909.04761). Here is the accompanying [repository](https://github.com/n-waves/multifit).\n",
    "\n",
    "The archive we are about to download contains wikipedia dumps for 8 languages - that can come in handy for our experiments with translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastai\n",
    "from fastai.text.all import *\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.12'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-16 14:14:38--  https://www.dropbox.com/sh/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki?dl=1\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /sh/dl/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki [following]\n",
      "--2020-09-16 14:14:38--  https://www.dropbox.com/sh/dl/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucb3ebffe3113434a8309a843dd3.dl.dropboxusercontent.com/zip_download_get/Ai7iI08_FU4ekjhkC66hv8r6tc5TVte4yHKYvPmUNzDrwwJOXDtG8FrLj6TAf7uq-T79oEbydWScPRqgrtlTIvjcAup3z_azZ9GCxSI_ebfSrw?dl=1 [following]\n",
      "--2020-09-16 14:14:38--  https://ucb3ebffe3113434a8309a843dd3.dl.dropboxusercontent.com/zip_download_get/Ai7iI08_FU4ekjhkC66hv8r6tc5TVte4yHKYvPmUNzDrwwJOXDtG8FrLj6TAf7uq-T79oEbydWScPRqgrtlTIvjcAup3z_azZ9GCxSI_ebfSrw?dl=1\n",
      "Resolving ucb3ebffe3113434a8309a843dd3.dl.dropboxusercontent.com (ucb3ebffe3113434a8309a843dd3.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
      "Connecting to ucb3ebffe3113434a8309a843dd3.dl.dropboxusercontent.com (ucb3ebffe3113434a8309a843dd3.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1840410681 (1.7G) [application/zip]\n",
      "Saving to: ‘data/preprocessed_wiki_8langs.zip’\n",
      "\n",
      "data/preprocessed_w 100%[===================>]   1.71G  44.0MB/s    in 54s     \n",
      "\n",
      "2020-09-16 14:15:33 (32.6 MB/s) - ‘data/preprocessed_wiki_8langs.zip’ saved [1840410681/1840410681]\n",
      "\n",
      "Archive:  data/preprocessed_wiki_8langs.zip\n",
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n",
      " extracting: data/README             \n",
      " extracting: data/fr-100.tar.gz      \n",
      " extracting: data/en-100.tar.gz      \n",
      " extracting: data/it-100.tar.gz      \n",
      " extracting: data/es-100.tar.gz      \n",
      " extracting: data/ja-100.tar.gz      \n",
      " extracting: data/pl-100.tar.gz      \n",
      " extracting: data/ru-100.tar.gz      \n",
      " extracting: data/de-100.tar.gz      \n",
      " extracting: data/zh-100.tar.gz      \n"
     ]
    }
   ],
   "source": [
    "!wget -nc 'https://www.dropbox.com/sh/srfwvur6orq0cre/AAAQc36bcD17C1KM1mneXN7fa/data/wiki?dl=1' -O 'data/preprocessed_wiki_8langs.zip'\n",
    "!unzip 'data/preprocessed_wiki_8langs.zip' -d 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just grab english for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-100/\n",
      "en-100/en.wiki.valid.tokens\n",
      "en-100/en.wiki.train.tokens\n",
      "en-100/en.wiki.test.tokens\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf data/en-100.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data/en-100/train\n",
    "!mkdir data/en-100/valid\n",
    "\n",
    "!mv data/en-100/en.wiki.train.tokens data/en-100/train\n",
    "!mv data/en-100/en.wiki.valid.tokens data/en-100/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en.wiki.test.tokens  \u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls data/en-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only issue I see is that all the tokens for train and validation live in a single file. That is problematic - if we were to have mutliple small files, that would allow us to parallelize tokenization across CPU cores more easily.\n",
    "\n",
    "Let's split the data into smaller files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file_into_chunks(path, num_chunks=24):\n",
    "    txt = Path(path).read().split('\\n')\n",
    "    chunk_len = len(txt) // num_chunks\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        with open(Path(path).parent / f'{str(i).zfill(2)}.txt', \"w\") as text_file:\n",
    "            text_file.write('\\n'.join(txt[i*chunk_len:(i+1)*chunk_len]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.04 s, sys: 3.06 s, total: 7.09 s\n",
      "Wall time: 7.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "split_file_into_chunks('data/en-100/train/en.wiki.train.tokens')\n",
    "split_file_into_chunks('data/en-100/valid/en.wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00.txt  04.txt  08.txt  12.txt  16.txt  20.txt  en.wiki.train.tokens\r\n",
      "01.txt  05.txt  09.txt  13.txt  17.txt  21.txt\r\n",
      "02.txt  06.txt  10.txt  14.txt  18.txt  22.txt\r\n",
      "03.txt  07.txt  11.txt  15.txt  19.txt  23.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls data/en-100/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use functionality provided by `fastai` to tokenize the data (tokenization is an expensive operation so it is good to offload it and perform it once before training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.2 s, sys: 2.68 s, total: 53.9 s\n",
      "Wall time: 4min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Path('data/en-100_tok')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tokenize_folder('data/en-100/', folders=['train', 'valid'], n_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counter.pkl  lengths.pkl  \u001b[0m\u001b[01;34mtrain\u001b[0m/  \u001b[01;34mvalid\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls data/en-100_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00.txt  03.txt  06.txt  09.txt  12.txt  15.txt  18.txt  21.txt\r\n",
      "01.txt  04.txt  07.txt  10.txt  13.txt  16.txt  19.txt  22.txt\r\n",
      "02.txt  05.txt  08.txt  11.txt  14.txt  17.txt  20.txt  23.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls data/en-100_tok/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = pd.read_pickle('data/en-100_tok/counter.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple `vocab` - this will allow to go from embeddings to their meaning and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab():\n",
    "    def __init__(self, counter, max_words):\n",
    "        self.itos = ['xxunk']\n",
    "        self.itos += [tup[0] for tup in counter.most_common(max_words-1) if tup[1] != 'xxunk']\n",
    "        self.stoi = defaultdict(lambda: 0) # 0 corresponds to 'xxunk'\n",
    "        for i, w in enumerate(self.itos):\n",
    "            self.stoi[w] = i\n",
    "            \n",
    "    def numericalize_line(self, line):\n",
    "        return [self.stoi[token] for token in line.split()]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(counter, 50_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to take the data we generated and translate it from containing words to containing word ids (this will allow us to pull corresponding embedding vectors from the `Embedding` layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def numericalize_data(path):\n",
    "    '''\n",
    "    path - path to directory containing train examples\n",
    "    '''\n",
    "    token_idxs = []\n",
    "    for path in sorted(list(Path(path).iterdir())):\n",
    "            with open(path) as f:\n",
    "                for line in f:\n",
    "                    token_idxs += vocab.numericalize_line(line)\n",
    "    return torch.LongTensor(token_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 1.39 s, total: 31.4 s\n",
      "Wall time: 31.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_toks = numericalize_data('data/en-100_tok/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_toks = numericalize_data('data/en-100_tok/valid/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119268584, 253305)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_toks), len(valid_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data above, we could train any embedding model. Here I chose to train them via a simple langauge model.\n",
    "\n",
    "This might not be the best option for training embeddings, but I want to understand better how to train embeddings this way in preparation for the [planned work on audio](https://github.com/earthspecies/audio-embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    nbatch = data.size(0) // bsz\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 128\n",
    "\n",
    "train_data = batchify(train_toks, BS)\n",
    "val_data = batchify(valid_toks, BS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is organized `[tokens, examples]`, meaning the text flows down the 128 columns (128 in this case is our batch size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([931785, 128])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxunk',\n",
       " '=',\n",
       " 'xxmaj',\n",
       " 'valkyria',\n",
       " 'xxmaj',\n",
       " 'chronicles',\n",
       " 'xxrep',\n",
       " '3',\n",
       " 'i',\n",
       " '=']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.itos[i] for i in train_data[:, 0]][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's hack togather a simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_SZ = 300\n",
    "HIDDEN_SZ = 300\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(len(vocab), EMB_SZ)\n",
    "        self.rnn = nn.GRU(input_size=EMB_SZ, hidden_size=HIDDEN_SZ)\n",
    "        self.decoder = nn.Linear(HIDDEN_SZ, len(vocab))\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.emb(x)\n",
    "        x, hidden = self.rnn(x, hidden)\n",
    "        return self.decoder(x), hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters())\n",
    "        return weight.new_zeros(1, BS, HIDDEN_SZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT = 72\n",
    "LOG_INTERVAL = 5000 # in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a subset of the data is great for quick experiments as we try to get things to work!\n",
    "\n",
    "# train_data = train_data[:80000, :]\n",
    "# val_data = val_data[:1000, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training procedure is taken from a pytorch [LM example]. I made slight modifications to work with our architecture and for ease of running inside a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n",
    "\n",
    "# get_batch subdivides the source data into chunks of length BPTT.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(BPTT, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(vocab)\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, BPTT):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            data, targets = data.cuda(), targets.cuda()\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            total_loss += len(data) * criterion(output.view(-1, len(vocab)), targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(vocab)\n",
    "    hidden = model.init_hidden()\n",
    "\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, BPTT)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        data, targets = data.cuda(), targets.cuda()\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, len(vocab)), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1e-1)\n",
    "#         optimizer.step()\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-lr)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % LOG_INTERVAL == 0 and batch > 0:\n",
    "            cur_loss = total_loss / LOG_INTERVAL\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // BPTT, lr,\n",
    "                elapsed * 1000 / LOG_INTERVAL, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |  5000/12941 batches | lr 0.00 | ms/batch 106.37 | loss  4.36 | ppl    78.51\n",
      "| epoch   1 | 10000/12941 batches | lr 0.00 | ms/batch 106.51 | loss  3.91 | ppl    49.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 1378.78s | valid loss  3.85 | valid ppl    46.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |  5000/12941 batches | lr 0.00 | ms/batch 106.57 | loss  3.73 | ppl    41.79\n",
      "| epoch   2 | 10000/12941 batches | lr 0.00 | ms/batch 106.57 | loss  3.68 | ppl    39.63\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 1380.16s | valid loss  3.73 | valid ppl    41.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |  5000/12941 batches | lr 0.00 | ms/batch 106.61 | loss  3.61 | ppl    37.11\n",
      "| epoch   3 | 10000/12941 batches | lr 0.00 | ms/batch 106.60 | loss  3.59 | ppl    36.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 1380.33s | valid loss  3.68 | valid ppl    39.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |  5000/12941 batches | lr 0.00 | ms/batch 106.44 | loss  3.55 | ppl    34.90\n",
      "| epoch   4 | 10000/12941 batches | lr 0.00 | ms/batch 106.17 | loss  3.54 | ppl    34.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 1376.13s | valid loss  3.65 | valid ppl    38.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Exiting from training early\n"
     ]
    }
   ],
   "source": [
    "model = Model().cuda()\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr)\n",
    "best_val_loss = None\n",
    "EPOCHS = 100\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open('models/best_model_lower_lr.pth', 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            for param_group in optimizer.param_groups:\n",
    "                lr /= 4.0\n",
    "                param_group['lr'] = lr\n",
    "#             lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the most extensive training ever! But that is not a problem - I'm more interested in acquainting myself with all the pieces than getting a good result at this point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_pickle(vocab.stoi, 'data/stoi.pkl') # can't pickle defaultdict because I used a lambda :(\n",
    "# pd.to_pickle(vocab.itos, 'data/itos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = pd.read_pickle('data/itos.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing functionality we defined in an earlier notebook\n",
    "\n",
    "from embedding_gym.core import Embeddings, evaluate_monolingual_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "\n",
    "model = torch.load('models/best_model_lower_lr.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question type</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>capital-common-countries</td>\n",
       "      <td>0 / 506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>capital-world</td>\n",
       "      <td>1 / 4524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>currency</td>\n",
       "      <td>0 / 866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>city-in-state</td>\n",
       "      <td>0 / 2467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>family</td>\n",
       "      <td>18 / 506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gram1-adjective-to-adverb</td>\n",
       "      <td>0 / 992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gram2-opposite</td>\n",
       "      <td>1 / 812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gram3-comparative</td>\n",
       "      <td>7 / 1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gram4-superlative</td>\n",
       "      <td>1 / 1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gram5-present-participle</td>\n",
       "      <td>6 / 1056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gram6-nationality-adjective</td>\n",
       "      <td>6 / 1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gram7-past-tense</td>\n",
       "      <td>4 / 1560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gram8-plural</td>\n",
       "      <td>0 / 1332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gram9-plural-verbs</td>\n",
       "      <td>5 / 870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  question type    result\n",
       "0      capital-common-countries   0 / 506\n",
       "1                 capital-world  1 / 4524\n",
       "2                      currency   0 / 866\n",
       "3                 city-in-state  0 / 2467\n",
       "4                        family  18 / 506\n",
       "5     gram1-adjective-to-adverb   0 / 992\n",
       "6                gram2-opposite   1 / 812\n",
       "7             gram3-comparative  7 / 1332\n",
       "8             gram4-superlative  1 / 1122\n",
       "9      gram5-present-participle  6 / 1056\n",
       "10  gram6-nationality-adjective  6 / 1599\n",
       "11             gram7-past-tense  4 / 1560\n",
       "12                 gram8-plural  0 / 1332\n",
       "13           gram9-plural-verbs   5 / 870"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.002507163323782235\n",
      "Examples with missing words in the dictionary: 3554\n",
      "Total examples: 19544\n"
     ]
    }
   ],
   "source": [
    "embeddings = Embeddings(model.emb.weight.cpu().detach().numpy(), itos)\n",
    "evaluate_monolingual_embeddings(embeddings, lower=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than 1% accuracy. It's hard to understand these results - are our embeddings not trained well, or would embeddings trained in this fashion be of poor quality in general? Or are they still useful in some sense even though they do not retain the linear relationship between embeddings?\n",
    "\n",
    "An interesting propostion that could shed more light on this would be to fork this notebook and implement a training procedure as described in the [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781), using eithe the skipgram or CBOW methods. Better training of the embeddings could also be called for.\n",
    "\n",
    "Finally, it might be a good idea to download a pretrained language model on the WIKI corpus (such as the one [shared by fastai](https://docs.fast.ai/text.learner)) and see if these linear relationship hold in that embedding space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
