---

title: Title


keywords: fastai
sidebar: home_sidebar



nb_path: "01_goodness_of_monolingual_embeddings.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: 01_goodness_of_monolingual_embeddings.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="How-do-you-evaluate-monolingual-embeddings?">How do you evaluate monolingual embeddings?<a class="anchor-link" href="#How-do-you-evaluate-monolingual-embeddings?"> </a></h2><p>What constitutes a good embedding? It would be easy to provide a cursory answer - a good embedding is one that lends itself well to a downstream task. While 100% true and accurate, this answer does not allow us to speak to the goodness of embeddings directly, without having to train an additional model.</p>
<p>Another way to look at this would be to define that embeddings are valuable and useful to the extent that they encode information about the language and the world around us. This is in line with the reasoning behind why embeddings were created in the first place - we want to train our embeddings, or our language model, on vast amount of unlabeled text, in a way that encodes syntactic and semantic information that can give us a boost on a downstream task where we have labels but the dataset might be of limited size.</p>
<p>Taking the second definition, we can attempt to query our embeddings on textual examples and evaluate the accuracy of the anwsers. In its simplest form, we can perform algebraic operations in the embedding space ("king" - "man" + "woman" = ?) and use this as a mechanism for evaluation. While not without <a href="https://www.aclweb.org/anthology/W16-2503.pdf">issues</a>, this approach does allow us to say something about the structure of the trained embedding space.</p>
<p>To demonstrate this approach, let's use the embeddings from a classic, seminal paper <a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space paper</a> by Tomas Mikolov et al.</p>
<p>We can download the embeddings using fastai (they were originally shared by the authors <a href="https://code.google.com/archive/p/word2vec/">here</a>).</p>
<p>Please note - the file is 1.7 GB compressed.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.data.all</span> <span class="kn">import</span> <span class="n">untar_data</span>
<span class="n">embedding_path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="s1">&#39;https://storage.googleapis.com/text-embeddings/GoogleNews-vectors-negative300.bin.tar.gz&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's load the embeddings using <code>gensim</code>.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models.keyedvectors</span> <span class="kn">import</span> <span class="n">KeyedVectors</span>

<span class="n">gensim_embeddings</span> <span class="o">=</span> <span class="n">KeyedVectors</span><span class="o">.</span><span class="n">load_word2vec_format</span><span class="p">(</span><span class="n">embedding_path</span><span class="p">,</span> <span class="n">binary</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">gensim_embeddings</span><span class="o">.</span><span class="n">index2entity</span><span class="p">),</span> <span class="n">gensim_embeddings</span><span class="p">[</span><span class="s1">&#39;cat&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(3000000, (300,))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>3 million distinct embeddings, each of dimensionality 300!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's perform the evaluation using the original list of <code>question-words.txt</code> as used in the paper (and that was shared by the authors on github <a href="https://github.com/tmikolov/word2vec/blob/master/questions-words.txt">here</a>).</p>
<p>We could use the functionality built into <code>gensim</code> to run the evaluation, but this might make it tricky to evaluate embeddings that we train ourselves, or should we want to modify the list of queries.</p>
<p>Instead, let's perform the evaluation using code that we develop in this repository. As a starting point, all we need is an array of embeddings and a list with words corresponding to each vector!</p>
<!-- We will use [annoy](https://github.com/spotify/annoy) for approximate nearest neighbor lookup. Upon the first run, the embeddings will be added to an index and multiple trees enabling the search will be constructed. Given the size of these embeddings, this took around 5 minutes for me.  -->
</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Embeddings" class="doc_header"><code>class</code> <code>Embeddings</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Embeddings</code>(<strong><code>embeddings</code></strong>, <strong><code>index2word</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gensim_embeddings</span><span class="o">.</span><span class="n">vectors</span><span class="p">[:</span><span class="mi">30000</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(30000, 300)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># let&#39;s stick to just 50_000 of the most popular words so that the computation will run faster</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">Embeddings</span><span class="p">(</span><span class="n">gensim_embeddings</span><span class="o">.</span><span class="n">vectors</span><span class="p">[:</span><span class="mi">50_000</span><span class="p">],</span> <span class="n">gensim_embeddings</span><span class="o">.</span><span class="n">index2word</span><span class="p">[:</span><span class="mi">50_000</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have the Embeddings in place, we can run some examples. France is to Paris as ? is to Warsaw...</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">embeddings</span><span class="o">.</span><span class="n">analogy</span><span class="p">(</span><span class="s1">&#39;France&#39;</span><span class="p">,</span> <span class="s1">&#39;Paris&#39;</span><span class="p">,</span> <span class="s1">&#39;Warsaw&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 108 ms, sys: 24 ms, total: 132 ms
Wall time: 33.3 ms
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;Poland&#39;, &#39;Polish&#39;, &#39;Romania&#39;, &#39;Lithuania&#39;, &#39;Poles&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Got that one right! Now let's try the classic example of king - man + women = ?</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>
<span class="n">embeddings</span><span class="o">.</span><span class="n">analogy</span><span class="p">(</span><span class="s1">&#39;king&#39;</span><span class="p">,</span> <span class="s1">&#39;man&#39;</span><span class="p">,</span> <span class="s1">&#39;woman&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CPU times: user 32 ms, sys: 0 ns, total: 32 ms
Wall time: 7.17 ms
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[&#39;queen&#39;, &#39;monarch&#39;, &#39;princess&#39;, &#39;prince&#39;, &#39;kings&#39;]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We get it right as well!</p>
<p>Despite kings and queens not being discussed that often in the news today, this is still a great and slightly unexpected performance. Why should such an algebraic structure emerge when trained on a lot of text data in the first place? But yet it does!</p>
<p>Let's explore the performance further, by running through the list of question-answer pairs.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="evaluate_monolingual_embeddings" class="doc_header"><code>evaluate_monolingual_embeddings</code><a href="__main__.py#L6" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>evaluate_monolingual_embeddings</code>(<strong><code>embeddings</code></strong>, <strong><code>lower</code></strong>=<em><code>False</code></em>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%time</span>

<span class="n">evaluate_monolingual_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>question type</th>
      <th>result</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>capital-common-countries</td>
      <td>397 / 506</td>
    </tr>
    <tr>
      <th>1</th>
      <td>capital-world</td>
      <td>1512 / 4524</td>
    </tr>
    <tr>
      <th>2</th>
      <td>currency</td>
      <td>64 / 866</td>
    </tr>
    <tr>
      <th>3</th>
      <td>city-in-state</td>
      <td>463 / 2467</td>
    </tr>
    <tr>
      <th>4</th>
      <td>family</td>
      <td>377 / 506</td>
    </tr>
    <tr>
      <th>5</th>
      <td>gram1-adjective-to-adverb</td>
      <td>257 / 992</td>
    </tr>
    <tr>
      <th>6</th>
      <td>gram2-opposite</td>
      <td>257 / 812</td>
    </tr>
    <tr>
      <th>7</th>
      <td>gram3-comparative</td>
      <td>1044 / 1332</td>
    </tr>
    <tr>
      <th>8</th>
      <td>gram4-superlative</td>
      <td>644 / 1122</td>
    </tr>
    <tr>
      <th>9</th>
      <td>gram5-present-participle</td>
      <td>745 / 1056</td>
    </tr>
    <tr>
      <th>10</th>
      <td>gram6-nationality-adjective</td>
      <td>1401 / 1599</td>
    </tr>
    <tr>
      <th>11</th>
      <td>gram7-past-tense</td>
      <td>1139 / 1560</td>
    </tr>
    <tr>
      <th>12</th>
      <td>gram8-plural</td>
      <td>876 / 1332</td>
    </tr>
    <tr>
      <th>13</th>
      <td>gram9-plural-verbs</td>
      <td>570 / 870</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Accuracy: 0.4986696684404421
Examples with missing words in the dictionary: 3175
Total examples: 19544
CPU times: user 5min 54s, sys: 3.6 s, total: 5min 57s
Wall time: 1min 29s
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This is a very good result - bear in mind that we are limiting ourselves to top@1 accuracy and that we are counting synonyms as failure!</p>
<p>Another consideration is that while word2vec embeddings are ingenious in how efficient they are to train, they are a relatively simple way of encoding information about a language.</p>
<p>Still, it is remarkable that embedding spaces posses the quality that allows us to perform operations such as the above!</p>

</div>
</div>
</div>
</div>
 

