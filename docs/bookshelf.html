---

title: Embedding Gym bookshelf

keywords: fastai
sidebar: home_sidebar

summary: "readings particularly relevant to working with embeddings"
description: "readings particularly relevant to working with embeddings"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: bookshelf.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><a href="http://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a> - a very good introduction to the Word2Vec family of models</p>
<p><a href="https://arxiv.org/abs/1301.3781">Word2Vec: Efficient Estimation of Word Representations in Vector Space</a> - training of embeddings using two takes on utilizing local context window - continuous bag of words (CBOW) and Skip-gram</p>
<p><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> - a discussion of what it means to obtain a vector space with meaningful substructure, training of embeddings that utilizes both local context and global statistics (co-occurence counts across entire dataset)</p>
<p><a href="https://arxiv.org/abs/1607.04606">FastText: Enriching Word Vectors with Subword Information</a> - embeddings that incorporate subword information</p>
<p><a href="https://arxiv.org/abs/1710.04087">Word Translation Without Parallel Data</a> - alignment of multilingual FastText embeddings with a deep adverserial network</p>

</div>
</div>
</div>
</div>
 

