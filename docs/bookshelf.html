---

title: Embedding Gym bookshelf


keywords: fastai
sidebar: home_sidebar

summary: "readings particularly relevant to working with embeddings"
description: "readings particularly relevant to working with embeddings"
nb_path: "bookshelf.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: bookshelf.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Seminal-papers">Seminal papers<a class="anchor-link" href="#Seminal-papers"> </a></h2><p><a href="http://jalammar.github.io/illustrated-word2vec/">The Illustrated Word2vec</a> - a very good introduction to the Word2Vec family of models</p>
<p><a href="https://arxiv.org/abs/1301.3781">Word2Vec: Efficient Estimation of Word Representations in Vector Space</a> - training of embeddings using two takes on utilizing local context window - continuous bag of words (CBOW) and Skip-gram</p>
<p><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a> - a discussion of what it means to obtain a vector space with meaningful substructure, training of embeddings that utilizes both local context and global statistics (co-occurence counts across entire dataset)</p>
<p><a href="https://arxiv.org/abs/1607.04606">FastText: Enriching Word Vectors with Subword Information</a> - embeddings that incorporate subword information</p>
<p><a href="https://arxiv.org/abs/1710.04087">Word Translation Without Parallel Data</a> - alignment of multilingual FastText embeddings with a deep adverserial network</p>
<h2 id="Furhter-reading">Furhter reading<a class="anchor-link" href="#Furhter-reading"> </a></h2><p><a href="https://aclweb.org/aclwiki/Analogy_(State_of_the_art">Listing and discussion of analogy datasets</a>)</p>
<p><a href="https://www.aclweb.org/anthology/W16-2503.pdf">Issues in evaluating semantic spaces using word analogies</a> - discussion of evaluation of semantic spaces using word analogies (these methods, while still useful - especially with suggested extensions - do not capture the quality of a set of embeddings very accurately)</p>

</div>
</div>
</div>
</div>
 

