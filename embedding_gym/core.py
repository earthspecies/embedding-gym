# AUTOGENERATED! DO NOT EDIT! File to edit: 01_goodness_of_monolingual_embeddings.ipynb (unless otherwise specified).

__all__ = ['Embeddings', 'evaluate_monolingual_embeddings', 'Dictionary']

# Cell
import numpy as np

class Embeddings():
    def __init__(self, embeddings, index2word):
        '''embeddings - numpy array of embeddings, index2word - list of words corresponding to embeddings'''
        assert len(embeddings) == len(index2word)
        self.vectors = embeddings
        self.i2w = index2word
        self.w2i = {w:i for i, w in enumerate(index2word)}

    def analogy(self, a, b, c, n=5, discard_question_words=True):
        '''
        a is to b as c is to ?

        Performs the following algebraic calculation: result = emb_a - emb_b + emb_c
        Looks up n closest words to result.

        Implements the embedding space math behind the famous word2vec example:
        king - man + woman = queen
        '''
        question_word_indices = [self.w2i[word] for word in [a, b, c]]
        a, b, c = [self.vectors[idx] for idx in question_word_indices]
        result = a - b + c

        if discard_question_words: return self.nn_words_to(result, question_word_indices, n)
        else:                      return self.nn_words_to(result, n=n)

    def nn_words_to(self, vector, skip_indices=[], n=5):
        nn_indices = self.word_idxs_ranked_by_cosine_similarity_to(vector)
        nn_words = []
        for idx in nn_indices:
            if idx in skip_indices: continue
            nn_words.append(self.i2w[idx])
            if len(nn_words) == n: break

        return nn_words

    def word_idxs_ranked_by_cosine_similarity_to(self, vector):
        return np.flip(
            np.argsort(self.vectors @ vector / (self.vectors_lengths() * np.linalg.norm(vector, axis=-1)))
        )

    def vectors_lengths(self):
        if not hasattr(self, 'vectors_length_cache'):
            self.vectors_length_cache = np.linalg.norm(self.vectors, axis=-1)
        return self.vectors_length_cache

    def __getitem__(self, word):
        return self.vectors[self.w2i[word]]

    @classmethod
    def from_txt_file(cls, path_to_txt_file, limit=None):
        '''create embeddings from word2vec embeddings text file'''
        index, vectors = [], []
        with open(path_to_txt_file) as f:
            f.readline() # discarding the header line
            for line in f:
                try:
                    embedding = np.array([float(s) for s in line.split()[1:]])
                    if embedding.shape[0] != 300: continue
                    vectors.append(embedding)
                    index.append(line.split()[0])
                except ValueError: pass # we may have encountered a 2 word embedding, for instance 'New York' or 'w dolinie'
                if limit is not None and len(vectors) == limit: break
        return cls(np.stack(vectors), index)

# Cell

from collections import defaultdict
import pandas as pd

def evaluate_monolingual_embeddings(embeddings, lower=False):
    with open('data/questions-words.txt') as f:
        lines = f.readlines()

    total_seen = defaultdict(lambda: 0)
    correct = defaultdict(lambda: 0)
    question_types = []
    not_found = 0

    for line in lines:
        if line[0] == ':':
            question_types.append(line[1:].strip())
            current_type = question_types[-1]
        else:
            total_seen[current_type] += 1
            example = line.strip().split(' ')
            if lower: example = [word.lower() for word in example]
            try:
                result = embeddings.analogy(*example[:2], example[3], 1)
                if example[2] == result[0]: correct[current_type] += 1
            except KeyError:
                not_found += 1

    types = []
    results = []
    for key in total_seen.keys():
        types.append(key)
        results.append(f'{correct[key]} / {total_seen[key]}')

    df = pd.DataFrame(data={'question type': types, 'result': results})
    display(df)
    print('Accuracy:', sum(correct.values()) / sum(total_seen.values()))
    print('Examples with missing words in the dictionary:', not_found)
    print('Total examples:', sum(total_seen.values()))

# Comes from 02_goodness_of_embeddings_across_languages.ipynb, cell

class Dictionary():
    def __init__(self, path_to_dict):
        self.read_words(path_to_dict)
        self.create_dict()

    def read_words(self, path_to_dict):
        source_words, target_words = [], []
        with open(path_to_dict) as f:
            for line in f.readlines():
                src, target = line.strip().split()
                source_words.append(src)
                target_words.append(target)
        self.source_words = source_words
        self.target_words = target_words

    def create_dict(self):
        self.dict = {}
        for src, target in zip(self.source_words, self.target_words):
            self.dict[src] = target

    def __getitem__(self, source_word):
        return self.dict[source_word]

    def __len__(self):
        return len(self.source_words)